import geopandas
import math

import pandas as pd
from arcgis.features import FeatureLayerCollection, GeoAccessor
from celery import group
from sqlalchemy import create_engine
from .sql import create_db_conn_from_envs, get_sql_alchemy_engine_from_envs, get_count
from .swizzle import swizzle_service
from .sweri_logging import log_this, logging
from worker import app
from arcgis.gis import GIS

global_engine = None

# @log_this
def get_view_data_source_id(view):
    view_flc = FeatureLayerCollection.fromitem(view)

    if len(view_flc.manager.properties.get('adminServiceInfo').get('serviceSource')) == 1:
        service_sources = view_flc.manager.properties.get('adminServiceInfo').get('serviceSource')
        source_id = next(iter(service_sources)).get('serviceItemId')

    else:
        raise ValueError(
            f"{len(view_flc.manager.properties.get('adminServiceInfo').get('serviceSource'))} sources returned, 1 expected")

    return source_id


def gdf_to_features(df, has_shape=True):
    # Convert DataFrame or GeoDataFrame to SEDF to featureset
    sdf = GeoAccessor.from_geodataframe(df) if has_shape else df
    features_to_add = sdf.spatial.to_featureset().features

    return features_to_add

def build_postgis_chunk_query(schema, table, object_ids):

    object_ids_str = ", ".join(str(i) for i in object_ids)

    query = f"""
        SELECT * FROM {schema}.{table}
        WHERE
        objectid IN ({object_ids_str});
    """
    return query


def postgis_query_to_gdf(pg_query, conn, geom_field='shape', drop_cols=['objectid', 'gdb_geomattr_data']):
    if geom_field is None:
        df = pd.read_sql(pg_query, conn.connection)
    else:
        df = geopandas.read_postgis(pg_query, conn.connection, geom_col=geom_field)
    # dropping objectid since it will be autogenerated, avoids double fields
    if not df.empty:
        df.drop(columns=drop_cols, inplace=True, errors='ignore')

    return df


# @log_this
def retry_upload(func, *args, **kwargs):
    chunk = args[3]
    if chunk > 1:
        new_chunk = int(chunk / 2)
        new_args = list(args)
        new_args[3] = new_chunk
        return func(*new_args, **kwargs)
    else:
        raise Exception('Upload Tries Exceeded')


def refresh_gis(gis_url, gis_user, gis_password):
    gis = GIS(gis_url,gis_user, gis_password)
    return gis

def get_object_id_chunks(conn, schema, table, where, chunk_size=500):
    ids = get_object_ids(conn, schema, table, where)

    if chunk_size == 1:
        for id in ids:
            yield [id]
        return

    for i in range(0, len(ids), chunk_size):
        yield ids[i:i + chunk_size]

def verify_feature_count(conn,schema, table, new_source_feature_layer):
    # check counts before swizzle
    db_count = get_count(conn, schema, table)
    fl_count = new_source_feature_layer.query(return_count_only=True)
    diff = abs(db_count - fl_count)
    percent_diff = diff / db_count
    threshold = 0.01  # 1 percent difference allowed
    if percent_diff > threshold:
        raise ValueError(f"Data source count mismatch after upload. Database count: {db_count}, Feature Layer count: {fl_count}, Difference: {diff} ({percent_diff*100:.2f}%)")

@log_this
def hosted_upload_and_swizzle(root_url, gis_url, gis_user, gis_password, view_id, source_feature_layer_ids, schema, table, max_points_before_single_geom_chunk, chunk_size, shape=True, drop_cols=['objectid', 'gdb_geomattr_data']):
    # setup new layer connection
    gis_con = refresh_gis(gis_url, gis_user, gis_password)
    view_item = gis_con.content.get(view_id)
    current_data_source_id = get_view_data_source_id(view_item)

    new_data_source_id = next(id for id in source_feature_layer_ids if id != current_data_source_id)
    new_source_feature_layer = get_feature_layer_from_item(gis_url, gis_user, gis_password, new_data_source_id)
    new_source_feature_layer.manager.truncate()
    conn = create_db_conn_from_envs()
    # list of tasks
    t = []
    if not shape:
        # attribute only table, no geometry
        for chunk_ids in get_object_id_chunks(conn, schema, table, f'1=1', chunk_size):
            t.append(upload_chunk_to_feature_layer.s(gis_url, gis_user, gis_password, new_data_source_id, schema, table,
                                                     chunk_ids, False, drop_cols))

    else:

        for chunk_ids in get_object_id_chunks(conn, schema, table, f'ST_NPoints(shape) > {max_points_before_single_geom_chunk}', 1):
            t.append(upload_chunk_to_feature_layer.s(gis_url, gis_user, gis_password, new_data_source_id, schema, table,
                                                     chunk_ids, shape, drop_cols))

        for chunk_ids in get_object_id_chunks(conn, schema, table, f'ST_NPoints(shape) <= {max_points_before_single_geom_chunk}', chunk_size):
            t.append(upload_chunk_to_feature_layer.s(gis_url, gis_user, gis_password, new_data_source_id, schema, table,
                                                     chunk_ids, shape, drop_cols))
    g = group(t)()
    g.get()
    # refreshing old references before swizzle service
    view_item = gis_con.content.get(view_id)
    new_source_item = gis_con.content.get(new_data_source_id)
    token = gis_con.session.auth.token

    verify_feature_count(conn, schema, table, new_source_feature_layer)

    swizzle_service(root_url, view_item.name, new_source_item.name, token)

    return new_source_item.name

def get_feature_layer_from_item(gis_url, gis_user, gis_password,  new_data_source_id):
    gis_con = refresh_gis(gis_url, gis_user, gis_password)
    new_source_item = gis_con.content.get(new_data_source_id)
    if (len(new_source_item.layers)) == 1:
        new_source_feature_layer = next(iter(new_source_item.layers))
    elif (len(new_source_item.tables)) == 1:
        new_source_feature_layer = next(iter(new_source_item.tables))
    else:
        raise ValueError(f"{len(new_source_item.layers)} sources returned, 1 expected")

    return new_source_feature_layer

def get_object_ids(conn, schema, table, where = '1=1'):
    q = f"SELECT objectid FROM {schema}.{table} WHERE {where};"
    cursor = conn.cursor()
    try:
        with conn.transaction():
            cursor.execute(q)
            results = cursor.fetchall()
            ids = [r[0] for r in results]

    except Exception as err:
        logging.error(f'error getting objectids: {err}, {q}')
        raise err
    return ids


@app.task()
def upload_chunk_to_feature_layer(gis_url, gis_user, gis_password, new_source_id, schema, table, object_ids, has_shape, drop_cols):

    global global_engine
    if global_engine is None:
        global_engine = get_sql_alchemy_engine_from_envs()

    try:
        feature_layer = get_feature_layer_from_item(gis_url, gis_user, gis_password, new_source_id)
        sql_query = build_postgis_chunk_query(schema, table, object_ids)

        with global_engine.connect() as engine_conn:
            features_gdf = postgis_query_to_gdf(sql_query, engine_conn, None if not has_shape else 'shape', drop_cols)

        features = gdf_to_features(features_gdf, has_shape)
        for feature in features:
            for key, value in feature.attributes.items():
                if isinstance(value, float) and math.isnan(value):
                    feature.attributes[key] = None

        response = feature_layer.edit_features(adds=features, rollback_on_failure=False)

        for index, feature in enumerate(response['addResults']):
            att = features[index].attributes

            if not feature.get("success", False):
                logging.warning(f"This feature could not be inserted: attributes={att}")


    except Exception as e:
        logging.error(f'error uploading chunk to feature layer: {e}, {schema}.{table}, ids: {object_ids}')
